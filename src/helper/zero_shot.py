import json
import helper.word_embeddings
from collections import OrderedDict, defaultdict
from nltk import word_tokenize, UnigramTagger
from nltk.corpus import brown

## Using the expressions and stored predictions generated by the LSTM and the interface to the word embeddings, this
# script performs the exchange of single words in the referring expressions by a new word (combined from the candidates).
# This requires loading the related files, parsing the expressions for a noun and replacing the word by a combination.
# Simultaneously with the replacing, variables are used to count the number of correct new words (hit@k accuracies).
# In addition, baseline files are generated which count the number of times a word is predicted by a model.

class Zero_Shooter:

    def __init__(self, modelpath, candidates):
        self.modelpath = modelpath
        self.bus_counter = 0
        with open(modelpath + 'all_highest_probs_'+ str(candidates) + '.json', 'r') as f:
            self.candidates = json.load(f)
        with open(modelpath + 'inject_refcoco_refrnn_compositional_3_512_1/4eval_greedy.json', 'r') as f:# 'restoredmodel_refs_greedy.json') as f: restoredmodel_refs_greedy/4eval_greedy
            self.refs = json.load(f)
        self.words_that_are_names = list()
        with open("./noun_list_long.txt", 'r') as f:
            for row in f.readlines():
               self.words_that_are_names.append(row.strip())
        self.unigram_tagger = UnigramTagger(brown.tagged_sents())
        self.zero_shot_refs = defaultdict()
        self.non_noun_counter = 0
        self.baseline_top_1 = defaultdict()
        self.baseline_top_5 = defaultdict()
        self.baseline_top_10 = defaultdict()

    # read and sort the candidates for a position in the sequence
    def get_predictions(self, region_id):
        predictions = list()
        tmp_dict = self.candidates[region_id]
        sorted_tmp = OrderedDict(sorted(tmp_dict.items(), key=lambda t: t[0]))
        for entry in sorted_tmp:
            predictions.append(sorted_tmp[entry][-1][0])
        return predictions

    # parse a sequence with the noun-list method
    def parse_for_names(self, predicted_words, cat):
        for i, word in enumerate(predicted_words):
            if word == str(cat):
                self.bus_counter += 1
            if word in self.words_that_are_names:  ## always returns first instance ...
                return i
        return -1

    # parse a sequence with a POS-tagger
    def parse_pos(self, tokens, cat):
        tags = self.unigram_tagger.tag(tokens)
        nouns = [x for x in tags if x[1] == 'NN']
        if len(nouns) > 0:
            if nouns[0][0] == cat:
                self.bus_counter += 1
            return tokens.index(nouns[0][0])  # to keep it easy - if two nouns, this is a simplification!
        else:
            unknown_nouns = [x for x in tags if x[1] == 'None']
            if len(unknown_nouns) > 0:
                return tokens.index(unknown_nouns[0])
            else:
                return -1

    # apply the parsing, word combination and exchange of a word to the test set,
    # count the hit@k accuracies and the frequency of predicitions (baseline).
    # The parsing is currently done with the noun-list, but can be changed to POS-tagger
    def do_zero_shot(self, embeddings, category, use_reduced_vector_space):
        self.word_changed_counter = 0
        self.zero_shot_counter = 0
        self.word_counter = 0
        category = str(category)
        hit_at_1 = 0
        hit_at_2 = 0
        hit_at_5 = 0
        hit_at_10 = 0
        #with open("/mnt/Data/zero_shot_reg/src/eval/new_models/with_reduced_cats_all/vocab_list.txt", 'r') as f:
           # vocab = f.read().splitlines()

        for region_id in self.candidates:

            region_id = str(region_id)
            sentence = self.get_predictions(region_id)
            self.word_counter += len(sentence)

            ## use pos tagger
            #index = self.parse_pos(sentence, category)

            ## OR use name list
            index = self.parse_for_names(sentence, category)

            if index < 0:
                self.zero_shot_refs[region_id] = self.refs[region_id]
                continue

            candidate_words_and_probs = self.candidates[region_id][str(index + 1)]
            cand_words = [x[0] for x in candidate_words_and_probs]
            cand_probs = [float(x[1]) for x in candidate_words_and_probs]

            new_vec = embeddings.words2embedding_weighted(cand_words, cand_probs, use_reduced_vector_space)
            if new_vec is not None:
                new_words_10 = embeddings.get_words_for_vector(new_vec, 10, use_reduced_vector_space)
                new_words_5 = embeddings.get_words_for_vector(new_vec, 5, use_reduced_vector_space)
                new_words_2 = embeddings.get_words_for_vector(new_vec, 2, use_reduced_vector_space)
                new_words_1 = embeddings.get_words_for_vector(new_vec, 1, use_reduced_vector_space)

                ##### generate baselines for comparison with WAC  ####
                for x in new_words_1:
                    if x[0] in self.baseline_top_1:
                        self.baseline_top_1[x[0]] += 1
                    else:
                        self.baseline_top_1[x[0]] = 1

                for x in new_words_5:
                    if x[0] in self.baseline_top_5:
                        self.baseline_top_5[x[0]] += 1
                    else:
                        self.baseline_top_5[x[0]] = 1

                for x in new_words_10:
                    if x[0] in self.baseline_top_10:
                        self.baseline_top_10[x[0]] += 1
                    else:
                        self.baseline_top_10[x[0]] = 1
                ######################################################

               # for x in new_words_10:
               #     if not x[0] in vocab:
               #         print "**************", x
               # for x in new_words_1:
               #     if not x[0] in vocab:
               #         print "***********************", x  # code to test whether out-of-vocabulary words appear at all

                if category in [x[0] for x in new_words_10]:
                    hit_at_10 += 1
                if category in [x[0] for x in new_words_5]:
                    hit_at_5 += 1
                if category in [x[0] for x in new_words_1]:
                    hit_at_1 += 1
                if category in [x[0] for x in new_words_2]:
                    hit_at_2 += 1

                if not new_words_1[0][0] in self.words_that_are_names:
                    self.non_noun_counter += 1
                #print self.words_that_are_names
                #    print new_words_1[0][0]

                ref = self.refs[region_id][0].split()
                #print ref

                self.zero_shot_counter += 1
                if not new_words_1[0][0] == ref[index]:
                    self.word_changed_counter += 1

                ref[index] = new_words_1[0][0]
                new_ref = ' '.join(ref)
                self.zero_shot_refs[region_id] = [new_ref]

        with open(self.modelpath  + 'baseline_frequencies_top1.json', 'w') as f:
            json.dump(self.baseline_top_1, f)
        with open(self.modelpath  + 'baseline_frequencies_top5.json', 'w') as f:
            json.dump(self.baseline_top_5, f)
        with open(self.modelpath  + 'baseline_frequencies_top10.json', 'w') as f:
            json.dump(self.baseline_top_10, f)

        print "non-nouns: ", self.non_noun_counter, " of ", len(self.candidates), " -> ", round(self.non_noun_counter / float(len(self.candidates))* 100, 2)
        return hit_at_1/ float(len(self.candidates)), hit_at_2/ float(len(self.candidates)), hit_at_5/ float(len(self.candidates)), \
               hit_at_10/ float(len(self.candidates)), len(self.candidates)

    # apply the method not to single nouns, but to all words of an expression (without parsing)
    # hit@k accuracies do not work here, because not all words are supposed to be the target word (only one in sequence)
    def do_zero_shot_all_words(self,embeddings, category, use_reduced_vector_space):

        self.word_counter = 0
        self.zero_shot_counter = 0
        self.word_changed_counter = 0

        category = str(category)
        for region_id in self.candidates:
            region_id = str(region_id)
            sentence = self.get_predictions(region_id)
            self.word_counter += len(sentence)

            for index, word in enumerate(sentence):
                candidate_words_and_probs = self.candidates[region_id][str(index + 1)]
                cand_words = [x[0] for x in candidate_words_and_probs]
                cand_probs = [float(x[1]) for x in candidate_words_and_probs]
                new_vec = embeddings.words2embedding_weighted(cand_words, cand_probs, use_reduced_vector_space)
                if new_vec is not None:
                    new_word = embeddings.get_words_for_vector(new_vec, 1, use_reduced_vector_space)

                    ref = self.refs[region_id][0].split()
                    ref[index] = new_word[0][0]
                    new_ref = ' '.join(ref)
                    self.zero_shot_refs[region_id] = [new_ref]
                    self.zero_shot_counter += 1
                    if not new_word[0][0] == word:
                        self.word_changed_counter += 1
                        print "____ ", region_id
                        print "original: ", sentence
                        print "after: ", new_ref

        return [],[],[],[],len(self.candidates)


if __name__ == '__main__':

    cats = ['laptop', 'bus', 'horse']
    #cats = ['all']
    use_reduced_vector_space = True    # vary: only words from vocab or full GloVe
    use_only_names = True               # vary: only noun candidates for zero-shot naming or all words in the space
    exchange_all_words = False          # vary: exchange all words or only the nouns (like normally)
    numbr_candidates = 10
    print "Number of vectors used for combination: ", numbr_candidates
    print "Reduced model: ", use_reduced_vector_space


    for c in cats:
        model = '/mnt/Data/zero_shot_reg/src/eval/model/with_reduced_cats_' + c + '/'
        zs = Zero_Shooter(model, numbr_candidates)
        embed = helper.word_embeddings.Embeddings(model, use_only_names)

        if use_reduced_vector_space:
            word_model = embed.init_reduced_embeddings()
        else:
            word_model = embed.get_global_model()
        print "vocab embeddings: ", len(word_model.vocab)

       # print zs.words_that_are_names
        print "**** ", c
        if exchange_all_words:
            results = zs.do_zero_shot_all_words(embed, c, use_reduced_vector_space)
        else:
            results =  zs.do_zero_shot(embed, c, use_reduced_vector_space)
        print "Exchanging all words (not only nouns): ", exchange_all_words
        print "number of utterances to analyse: ", len(zs.candidates)
      #  print "valid sentences:", results[4], ',', round(results[4]/float(len(zs.candidates)) * 100, 2), '%'
        print "( Number of embeddings: ", len(word_model.vocab), ")"
        chance_acc = 1 / float(len(word_model.vocab))
        print "chance: ",  round(chance_acc * 100, 4), '%'
#        print "accuracy hit@1: ", round(results[0] * 100, 2) , '%'
 #       print "accuracy hit@2: ", round(results[1] * 100, 2) , '%'
  #      print "accuracy hit@5: ", round(results[2] * 100, 2) , '%'
   #     print "accuracy hit@10: ", round(results[3] * 100, 2) , '%'
    #    print "correct hits before: ", round(zs.bus_counter / float(results[4]) * 100, 2), '%\n'

#        print "zero-shot words: ", zs.zero_shot_counter
 #       print "all words: ", zs.word_counter
#        print "Percentage: ", zs.zero_shot_counter / float(zs.word_counter)
        print "Word changed: ", zs.word_changed_counter / float(zs.zero_shot_counter)
        print "All words number: ", zs.word_counter
        print "All zero-shot procedures number: ", zs.zero_shot_counter
        print "% affected: ", round((zs.zero_shot_counter / float(zs.word_counter)) * 100, 2)

        if exchange_all_words:
            name = 'all'
        else:
            name = 'nouns'

        with open(model + 'zero_shot_refs_' + str(c) + '.json', 'w') as f:
            json.dump(zs.zero_shot_refs, f)







